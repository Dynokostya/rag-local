{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default chunking (if semantic doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"data/\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from config import azure_openai_key, azure_openai_endpoint, azure_openai_api_version, azure_openai_embedding_deployment\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\"data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# documents[0].page_content\n",
    "text_splitter = SemanticChunker(embeddings=embeddings,\n",
    "                                breakpoint_threshold_type='interquartile')\n",
    "\n",
    "chunks = text_splitter.create_documents([documents[i].page_content for i in range(len(documents))])\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings and save to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(persist_directory)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# vectorstore = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mchunks\u001b[49m, embeddings, persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# db = Chroma()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# db.from_documents\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from config import azure_openai_embedding_deployment\n",
    "from langchain_chroma import Chroma\n",
    "from config import azure_openai_key, azure_openai_endpoint, azure_openai_api_version\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "persist_directory = \"chroma\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "# vectorstore = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "# db = Chroma()\n",
    "# db.from_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create persist folder for reusing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Відповідь---\n",
      "У контексті не було надано інформації, шукаю по зовнішнім ресурсам.\n",
      "\n",
      "---Пояснення---\n",
      "В наданих документах не міститься інформації щодо \"Consistent evolvement\" \n",
      "або \"послідовного розвитку\". \n",
      "\n",
      "---Думки---\n",
      "1. Перевірив наданий контекст.\n",
      "2. Не знайшов згадки про \"Consistent evolvement\".\n",
      "3. Переходжу до використання зовнішніх ресурсів.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from config import azure_openai_gpt_deployment, azure_openai_api_version\n",
    "\n",
    "\n",
    "persist_directory = \"chroma\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "gpt = AzureChatOpenAI(\n",
    "    deployment_name=azure_openai_gpt_deployment,\n",
    "    api_version=azure_openai_api_version\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gpt,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "System:\n",
    "```\n",
    "You are a search assistant.\n",
    "You are an expert in \"Consistent evolvement\".\n",
    "You will be asked a question.\n",
    "Question may contain multiple answers.\n",
    "Provide the full text from the retrieved documents that supports your answer.\n",
    "\n",
    "All information will be provided in Ukrainian language.\n",
    "Answer in Ukrainian language.\n",
    "\n",
    "You should not use any external resources or make up information, but \n",
    "if you cannot find context for the answer, say \n",
    "\"У контексті не було надано інфморації, шукаю по зовнішнім ресурсам\" and use your knowledge and external recources.\n",
    "\n",
    "You should think step by step and give every though.\n",
    "When you wrote 10 words in 1 line, you should start a new line.\n",
    "Every line should contain no more than 10 words.\n",
    "Example:\n",
    "```\n",
    "---Відповідь---\n",
    "...\n",
    "---Пояснення---\n",
    "...\n",
    "---Думки---\n",
    "...\n",
    "```\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "user = \"\"\"\n",
    "User:\n",
    "```\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "query = prompt + user + \"\\nAnswer:\"\n",
    "answer = qa_chain.invoke({\"query\": query})\n",
    "print(answer.get(\"result\"))\n",
    "#print()\n",
    "#answer.get(\"source_documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
