{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main file for chunking and chatGPT prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dynokostya/Documents/Projects/rag-local/data/stalyi_rozv\n",
      "/home/dynokostya/Documents/Projects/rag-local/chroma_db/stalyi_rozv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "data_path = '/home/dynokostya/Documents/Projects/rag-local/data/stalyi_rozv'\n",
    "chroma_db_path = '/home/dynokostya/Documents/Projects/rag-local/chroma_db/stalyi_rozv'\n",
    "if not os.path.exists(chroma_db_path):\n",
    "    os.makedirs(chroma_db_path)\n",
    "print(data_path)\n",
    "print(chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default chunking (if semantic doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Select your directory\n",
    "loader = DirectoryLoader(data_path)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from config import azure_openai_key, azure_openai_endpoint, azure_openai_api_version, azure_openai_embedding_deployment\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "# Select your directory\n",
    "loader = DirectoryLoader(data_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Gradient does the best semantic chunking, alternative = 'percentile'\n",
    "text_splitter = SemanticChunker(embeddings=embeddings,\n",
    "                                breakpoint_threshold_type='gradient')\n",
    "\n",
    "chunks = text_splitter.create_documents([documents[i].page_content for i in range(len(documents))])\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and save to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from config import azure_openai_embedding_deployment\n",
    "from langchain_chroma import Chroma\n",
    "from config import azure_openai_key, azure_openai_endpoint, azure_openai_api_version\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = azure_openai_api_version\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "if not os.path.exists(chroma_db_path):\n",
    "    os.makedirs(chroma_db_path)\n",
    "\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o model usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for сталий розвиток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "System:\n",
    "```\n",
    "You are a search assistant.\n",
    "You are an expert in \"Consistent evolvement\".\n",
    "You will be asked a question.\n",
    "Question may contain multiple answers.\n",
    "You should always give an exact answer for the question.\n",
    "All information will be provided in Ukrainian language.\n",
    "Answer in Ukrainian language.\n",
    "\n",
    "If you cannot find the answer in context/documents, say \n",
    "\"Нічого не знайдено в документах. Використовую власні знання...\".\n",
    "Then, use your knowldege or external recources and always give an answer for the question.\n",
    "If needed, make more analysis.\n",
    "\n",
    "When you wrote 10 words in 1 line, you should start a new line.\n",
    "Every line should contain no more than 10 words.\n",
    "Write and structure your thoughts.\n",
    "Take a deep breath and think step by step.\n",
    "Explain each of your steps in detail (in ukrainian).\n",
    "\n",
    "Example:\n",
    "```\n",
    "---Відповідь---\n",
    "...\n",
    "---Пояснення---\n",
    "...\n",
    "---Думки---\n",
    "...\n",
    "```\n",
    "\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "System:\n",
    "```\n",
    "You are a search assistant.\n",
    "You are an expert in \"Intelectual ownership and patents\".\n",
    "You will be asked a question.\n",
    "Question may contain multiple answers.\n",
    "You should always give an exact answer for the question.\n",
    "All information will be provided in Ukrainian language.\n",
    "Answer in Ukrainian language.\n",
    "\n",
    "If you cannot find the answer in context/documents, say \n",
    "\"Нічого не знайдено в документах. Використовую власні знання...\".\n",
    "Then, use your knowldege or external recources and always give an answer for the question.\n",
    "If needed, make more analysis.\n",
    "\n",
    "When you wrote 10 words in 1 line, you should start a new line.\n",
    "Every line should contain no more than 10 words.\n",
    "Write and structure your thoughts.\n",
    "Take a deep breath and think step by step.\n",
    "Explain each of your steps in detail (in ukrainian).\n",
    "\n",
    "Example:\n",
    "```\n",
    "---Відповідь---\n",
    "...\n",
    "---Пояснення---\n",
    "...\n",
    "---Думки---\n",
    "...\n",
    "```\n",
    "\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Відповідь---\n",
      "Неправильно\n",
      "\n",
      "---Пояснення---\n",
      "День перевитрати Землі, відомий як Earth Overshoot Day, \n",
      "зазвичай настає значно раніше в році, ніж 28 жовтня. \n",
      "У попередні роки цей день припадав на липень-серпень.\n",
      "\n",
      "---Думки---\n",
      "1. День перевитрати Землі означає, що людство \n",
      "використало всі ресурси, які планета може відновити \n",
      "за рік.\n",
      "2. Цей день зазвичай визначається у середині року.\n",
      "3. Дата 28 жовтня є занадто пізньою для Earth \n",
      "Overshoot Day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from config import azure_openai_gpt_deployment, azure_openai_api_version\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=azure_openai_embedding_deployment,\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings)\n",
    "\n",
    "gpt = AzureChatOpenAI(\n",
    "    deployment_name=azure_openai_gpt_deployment,\n",
    "    api_version=azure_openai_api_version\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gpt,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "user = \"\"\"\n",
    "User:\n",
    "```\n",
    "День перевитрати Землі (англ. Earth overshoot day) 2024 року - 28 жовтня.\n",
    "\n",
    "Виберіть одну відповідь:\n",
    "Правильно\n",
    "Неправильно\n",
    "```\n",
    "\"\"\"\n",
    "query = prompt + user + \"\\nAnswer:\"\n",
    "answer = qa_chain.invoke({\"query\": query})\n",
    "print(answer.get(\"result\"))\n",
    "#print()\n",
    "#answer.get(\"source_documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
